{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Algoritmos de regresi√≥n\n",
    "## Requisitos\n",
    "## Introducci√≥n\n",
    "## Regresi√≥n lineal (OLS)\n",
    "### Definici√≥n\n",
    "Como sabemos, los modelos de conocimiento en aprendizaje supervisado pueden ser entendidos como funciones que reciben una entrada, la cual es un vector con los valores de los atributos, y devuelven una salida, que se trata de un valor estimado para\n",
    "la clase. El algoritmo OLS ‚Äì_ordinary least squares, en ingl√©s_ ‚Äìes capaz de generar un modelo que consta de una combinaci√≥n lineal de los atributos. En concreto, el modelo que genera OLS puede expresarse matem√°ticamente del siguiente modo:\n",
    "\n",
    "$$ùë¶ÃÇ=ùë§_0 + ùë§_1 ¬∑ ùë•_1 + ‚ãØ + ùë§_ùëù ¬∑ ùë•_ùëù$$\n",
    "\n",
    "En la anterior ecuaci√≥n, $ùë¶ÃÇ$ es el valor predicho por el algoritmo, $x_1, ..., x_p$ son los valores de los atributos, $ùë§_1, ..., ùë§_ùëù‚ààùëÖ$ son los coeficientes del modelo y $w_0$ es el t√©rmino independiente de la combinaci√≥n lineal.\n",
    "\n",
    "Una vez explicado el modelo de OLS, vamos a explicar la funci√≥n de coste que emplea para optimizar los coeficientes del modelo y as√≠, conseguir que sea capaz de estimar con mayor precisi√≥n la variable de salida. \n",
    "\n",
    "La funci√≥n objetivo o funci√≥n de coste es aquella que evalua c√≥mo de buena es la posible soluci√≥n. En el caso de OLS, la funci√≥n objetivo que usa es [MSE (mean squared error)](https://www.iartificial.net/error-cuadratico-medio-para-regresion/), y consiste en encontrar aquellos valores para los coeficientes $ùë§_0, ..., ùë§_ùëù$ que minimizan el error. \n",
    "![Reducici√≥n del error cuadr√°tico medio](https://miro.medium.com/max/1400/1*1pBRuJzxj4oqwnDx0Ygf1w.png \"Reducici√≥n del error cuadr√°tico medio\")\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La f√°cil interpretabilidad del modelo.\n",
    "+ La gran eficacia en problemas de √≠ndole lineal.\n",
    "+ El tiempo de ejecuci√≥n del entrenamiento muy razonable.\n",
    "+ El tiempo de ejecuci√≥n de la predicci√≥n casi instant√°neo.\n",
    "\n",
    "### Desventajas\n",
    "+ La independencia entre los atributos.\n",
    "+ La distribuci√≥n normal de los datos.\n",
    "+ La relaci√≥n lineal entre los atributos y la clase.\n",
    "\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi√≥n log√≠stica\n",
    "### Definici√≥n\n",
    "La regresi√≥n log√≠stica (LOGR) es un algoritmo de aprendizaje autom√°tico que se utiliza sobre todo para los problemas de clasificaci√≥n, aunque tambi√©n se puede emplear en problemas de regresi√≥n. Es un algoritmo de an√°lisis predictivo y se basa en el concepto de probabilidad. \n",
    "\n",
    "LOGR utiliza una funci√≥n definida como funci√≥n sigmoide y devuelve un valor entre 0 y 1 dependiendo del valor de entrada que reciba. Pero claro, esto s√≥lo es √∫til si tenemos que predecir dos valores, ¬øpero que pasar√≠a si tenemos que predecir _n_ valores? Para resolver este problema se lleva a cabo la descomposici√≥n del problema original en _m_ subproblemas, del tipo binario (la funci√≥n sigmoide). A este proceso se le define como _una contra todos o [one vs rest (OvR)](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)_.\n",
    "\n",
    "La divisi√≥n en subproblemas binarios de tipo _OvR_ permite realizar una regresi√≥n usando la regresi√≥n log√≠stica m√°s simple. Cada modelo, una vez entrenado, aprender√≠a una frontera lineal de decisi√≥n que enfrenta cada target con los restantes, tal como se muestra en la figura para un problema con 3 targets. \n",
    "\n",
    "![One vs rest (OvR)](../assets/img/OvR.png \"One vs rest (OvR)\")\n",
    "\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La f√°cil interpretabilidad de las predicciones.\n",
    "+ El tiempo de ejecuci√≥n del entrenamiento muy razonable.\n",
    "+ El tiempo de ejecuci√≥n de la predicci√≥n casi instant√°neo.Por otra parte, el algoritmo LOGR posee ciertos inconvenientes:‚Ä¢La independencia entre los atributos.‚Ä¢La excesiva simplicidad del modelo.\n",
    "### Desventajas\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √Årboles de decisi√≥n\n",
    "### Definici√≥n\n",
    "### Ventajas\n",
    "### Desventajas\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecinos m√°s cercanos\n",
    "### Definici√≥n\n",
    "El algoritmo de vecinos m√°s cercanos ‚Äì_k nearest neighbors(KNN), en ingl√©s_ ‚Äìse basa en la idea de que ejemplos similares en sus atributos presentan tambi√©n un comportamiento similar en el valor de sus clases. La idea consiste en producir como predicci√≥n el promedio de las clases de los ejemplos de entrenamiento m√°s parecidos (vecinos) al ejemplo de prueba que hay que predecir. Definiremos qu√© debe considerarse m√°s parecido, aspecto que afecta a la eficacia del algoritmo.\n",
    "\n",
    "Para llevar a cabo las predicciones, el algoritmo KNN cl√°sico se basa en los dos siguientes elementos configurables: el n√∫mero $ùëò‚ààùëÅ$ de vecinos m√°s cercanos y la funci√≥n de distancia d. Con respecto a esta √∫ltima, existen numerosas funciones de distancia propuestas en la literatura. En esta secci√≥n usaremos la distancia de Minkowski, la cual se define as√≠:$$Minkowski(ùëí_ùëñ,ùëí_ùëó)=(\\sum_{d=1}^{p}|x_{i,d} - x_{j,d}|^q)^\\frac{1}{q}$$\n",
    "\n",
    "La funci√≥n de distancia es un hiperpar√°metro de KNN que afecta en gran medida a la bondad de las predicciones. Es cierto, que cada problema requiere una funci√≥n de distancia m√°s adecuada a la naturaleza y distribuci√≥n de los datos, pero en la mayor√≠a de los casos, la distancia m√°s utilizada es la eucl√≠dea, cuando el valor de p es igual a 2\n",
    "![Distancia de Minkowski](https://leovan.me/images/cn/2019-01-01-similarity-and-distance-measurement/2D-unit-balls.png)\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La f√°cil interpretabilidad de las predicciones.\n",
    "+ El tiempo de ejecuci√≥n del entrenamiento pr√°cticamente nulo.\n",
    "\n",
    "### Desventajas\n",
    "+ El tiempo de ejecuci√≥n alto en la fase de predicci√≥n.\n",
    "+ La falta de generalizaci√≥n.\n",
    "+ La necesidad de atributos relevantes y en igual escala.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}