{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Algoritmos de regresi√≥n\n",
    "## Tabla de contenidos\n",
    "+ [Introducci√≥n](#Introducci√≥n)\n",
    "+ [Entorno](#Entorno)\n",
    "+ [Regresi√≥n lineal m√∫ltiple](#OLS)\n",
    "+ [Regresi√≥n log√≠stica](#LOGR)\n",
    "+ [√Årboles de regresi√≥n](#RT)\n",
    "+ [Vecinos m√°s cercanos](#KNN)\n",
    "\n",
    "## Introducci√≥n<a name=\"Introducci√≥n\"></a>\n",
    "En este tutorial vamos a explicar los algoritmos m√°s comunes para problemas de regresi√≥n. Estos algoritmos se suelen emplear en proyectos de machine learning y concretamente en la parte de aprendizaje supervisado, aquellos en los que los datos est√°n etiquetados. \n",
    "\n",
    "Ahora bien, ¬øqu√© es un problema de regresi√≥n? Existen dos tipos de datos, los datos continuos y los datos discretos o categ√≥ricos. Los problemas de regresi√≥n se dan cuando tenemos que analizar un conjuntos de datos continuos, es decir, s datos que pertenecen al conjunto de los n√∫meros reales.\n",
    "\n",
    "Por otro lado, para continuar con el aprendizaje en el √°rea de la ciencia de datos, este tutorial forma parte del [punto 5. CRISP-DM: Modelado](https://www.adictosaltrabajo.com/2021/01/14/metodologia-crisp-dm/). Si no lo has le√≠do a√∫n, te recomiendo su lectura.\n",
    "\n",
    "Comencemos!\n",
    "\n",
    "## Entorno<a name=\"Entorno\"></a>\n",
    "El tutorial est√° escrito usando el siguiente entorno:\n",
    "\n",
    "+ Hardware: Port√°til MSI P65 Crator 8RD (2.2 GHz Intel Core i7, 16 GB RAM)\n",
    "+ Sistema operativo: Ubuntu 21.04\n",
    "+ Python 3.9\n",
    "+ Jupyter Notebook 6.4.6\n",
    "+ Scikit-learn 1.0.2\n",
    "\n",
    "\n",
    "## Regresi√≥n lineal m√∫ltiple (OLS)<a name=\"OLS\"></a>\n",
    "### Definici√≥n\n",
    "Como sabemos, los modelos de conocimiento en aprendizaje supervisado pueden ser entendidos como funciones que reciben una entrada, la cual es un vector con los valores de los atributos, y devuelven una salida, que se trata de un valor estimado para\n",
    "la clase. El algoritmo OLS ‚Äì_ordinary least squares, en ingl√©s_ ‚Äì es capaz de generar un modelo que consta de una combinaci√≥n lineal de los atributos. En concreto, el modelo que genera OLS puede expresarse matem√°ticamente del siguiente modo:\n",
    "\n",
    "$$ùë¶ÃÇ=ùë§_0 + ùë§_1 ¬∑ ùë•_1 + ‚ãØ + ùë§_ùëù ¬∑ ùë•_ùëù$$\n",
    "\n",
    "En la anterior ecuaci√≥n, $ùë¶ÃÇ$ es el valor predicho por el algoritmo, $x_1, ..., x_p$ son los valores de los atributos, $ùë§_1, ..., ùë§_ùëù‚ààùëÖ$ son los coeficientes del modelo y $w_0$ es el t√©rmino independiente de la combinaci√≥n lineal.\n",
    "\n",
    "Una vez explicado el modelo de OLS, vamos a explicar la funci√≥n de coste que emplea para optimizar los coeficientes del modelo y as√≠, conseguir que sea capaz de estimar con mayor precisi√≥n la variable de salida. \n",
    "\n",
    "La funci√≥n objetivo o funci√≥n de coste es aquella que evalua c√≥mo de buena es la posible soluci√≥n. En el caso de OLS, la funci√≥n objetivo que usa es [MSE (mean squared error)](https://www.iartificial.net/error-cuadratico-medio-para-regresion/), y consiste en encontrar aquellos valores para los coeficientes $ùë§_0, ..., ùë§_ùëù$ que minimizan el error. \n",
    "![Reducici√≥n del error cuadr√°tico medio](../assets/img/OLS.png \"Reducici√≥n del error cuadr√°tico medio\")\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La f√°cil interpretabilidad del modelo.\n",
    "+ La gran eficacia en problemas de √≠ndole lineal.\n",
    "+ El tiempo de ejecuci√≥n del entrenamiento muy razonable.\n",
    "+ El tiempo de ejecuci√≥n de la predicci√≥n casi instant√°neo.\n",
    "\n",
    "### Desventajas\n",
    "+ La independencia entre los atributos.\n",
    "+ La distribuci√≥n normal de los datos.\n",
    "+ La relaci√≥n lineal entre los atributos y la clase.\n",
    "\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de registros: 442\n",
      "N√∫mero de atributos (x1,..., xp): 10\n",
      "\n",
      "Valor del t√©rmino independiente (w0): 152.1334841628965\n",
      "Valor de los coeficientes (w1, ..., w10): [ -10.01219782 -239.81908937  519.83978679  324.39042769 -792.18416163\n",
      "  476.74583782  101.04457032  177.06417623  751.27932109   67.62538639]\n",
      "\n",
      "Datos que vamos a ofrecer al modelo -> [-0.00914709  0.05068012 -0.03961813 -0.04009932 -0.00844872  0.01622244\n",
      " -0.06549067  0.07120998  0.01776348 -0.06735141]\n",
      "\n",
      "Valor de la salida real: 124.0\n",
      "Predicci√≥n de la salida: 135.7 \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "# Dataset de ejemplo\n",
    "dataset = load_diabetes()\n",
    "\n",
    "# Comprender los datos\n",
    "print(f'N√∫mero de registros: {dataset.data.shape[0]}')\n",
    "print(f'N√∫mero de atributos (x1,..., xp): {dataset.data.shape[1]}')\n",
    "\n",
    "# Carga el algoritmo que se va a entrenar\n",
    "linear_regresion = linear_model.LinearRegression()\n",
    "\n",
    "# Entrenar del modelo\n",
    "trained_model = linear_regresion.fit(dataset.data, dataset.target)\n",
    "\n",
    "# Obtenci√≥n del t√©rmino independiente del modelo.\n",
    "print(f'\\nValor del t√©rmino independiente (w0): {trained_model.intercept_}')\n",
    "\n",
    "# Obtenci√≥n de los coeficientes del modelo.\n",
    "print(f'Valor de los coeficientes (w1, ..., w10): {trained_model.coef_}')\n",
    "\n",
    "# Una vez entrenado, vamos a proceder a probar que tal predice nuestro modelo cogiendo un registro del dataset\n",
    "row = 305\n",
    "example = dataset.data[row]\n",
    "target = dataset.target[row]\n",
    "pred = trained_model.predict([example])\n",
    "\n",
    "print(f'\\nDatos que vamos a ofrecer al modelo -> {example}\\n')\n",
    "print(f'Valor de la salida real: {target}')\n",
    "print(\"Predicci√≥n de la salida: %.1f \" % pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresi√≥n log√≠stica<a name=\"LOGR\"></a>\n",
    "### Definici√≥n\n",
    "La regresi√≥n log√≠stica (LOGR) es un algoritmo de aprendizaje autom√°tico que se utiliza sobre todo para los problemas de clasificaci√≥n, aunque tambi√©n se puede emplear en problemas de regresi√≥n. Es un algoritmo de an√°lisis predictivo y se basa en el concepto de probabilidad. \n",
    "\n",
    "LOGR utiliza una funci√≥n definida como funci√≥n sigmoide y devuelve un valor entre 0 y 1 dependiendo del valor de entrada que reciba. Pero claro, esto s√≥lo es √∫til si tenemos que predecir dos valores, ¬øpero que pasar√≠a si tenemos que predecir _n_ valores? Para resolver este problema se lleva a cabo la descomposici√≥n del problema original en _m_ subproblemas, del tipo binario (la funci√≥n sigmoide). A este proceso se le define como _una contra todos o [one vs rest (OvR)](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)_.\n",
    "\n",
    "La divisi√≥n en subproblemas binarios de tipo _OvR_ permite realizar una regresi√≥n usando la regresi√≥n log√≠stica m√°s simple. Cada modelo, una vez entrenado, aprender√≠a una frontera lineal de decisi√≥n que enfrenta cada target con los restantes, tal como se muestra en la figura para un problema con 3 targets. \n",
    "\n",
    "![One vs rest (OvR)](../assets/img/OvR.png \"One vs rest (OvR)\")\n",
    "\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La f√°cil interpretabilidad de las predicciones.\n",
    "+ El tiempo de ejecuci√≥n del entrenamiento muy razonable.\n",
    "+ El tiempo de ejecuci√≥n de la predicci√≥n casi instant√°neo.\n",
    "\n",
    "### Desventajas\n",
    "+ La independencia entre los atributos.\n",
    "+ La excesiva simplicidad del modelo.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √Årboles de regresi√≥n<a name=\"RT\"></a>\n",
    "### Definici√≥n\n",
    "Un √°rbol de decisi√≥n es un tipo de modelo que puede usarse tanto para clasificaci√≥n  como regresi√≥n. En este √∫ltimo caso, se denomina √°rbol de regresi√≥n, aunque su construcci√≥n es an√°loga a la del √°rbol de decisi√≥n. Los √°rboles est√°n compuestos por los siguientes elementos:\n",
    "+ Nodo ra√≠z. Es el nodo principal o primer nodo del √°rbol.\n",
    "+ Nodo. Son aquellos elementos del √°rbol donde se van tomando las decisiones para llegar a las hojas.\n",
    "+ Nodo hoja. Nodos que no tienen ning√∫n hijo y contienen la varible de salida (el dato que buscamos).\n",
    "\n",
    "Para la construci√≥n de los √°rboles de regresi√≥n se emplea el dataset de entrenamiento y el modelo va construyendo los nodos haciendo segmentaciones entre los atributos del conjunto de datos buscando reducir el error cuadr√°tico medio (MSE) hacia los niveles inferiores y asi obtener unos nodos hoja que contengan la informaci√≥n que buscaremos en las futuras predicciones.\n",
    "\n",
    "![√Årboles de decisi√≥n](https://miro.medium.com/max/1630/1*XZ220vTa7rN8ccJZZNe09w.png \"√Årboles de decisi√≥n\")\n",
    "  \n",
    "### Ventajas\n",
    "+ √ötil en √°reas en las que la relaci√≥n entre las variables no es lineal.\n",
    "+ Tiempo de ejecuci√≥n del entrenamiento razonable.\n",
    "\n",
    "### Desventajas\n",
    "+ Es propenso al sobreentrenamiento.\n",
    "+ La sensibilidad al desbalanceo de la variable de saldia.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecinos m√°s cercanos<a name=\"KNN\"></a>\n",
    "### Definici√≥n\n",
    "El algoritmo de vecinos m√°s cercanos ‚Äì_k nearest neighbors(KNN), en ingl√©s_ ‚Äìse basa en la idea de que ejemplos similares en sus atributos presentan tambi√©n un comportamiento similar en el valor de sus clases. La idea consiste en producir como predicci√≥n el promedio de las clases de los ejemplos de entrenamiento m√°s parecidos (vecinos) al ejemplo de prueba que hay que predecir. Definiremos qu√© debe considerarse m√°s parecido, aspecto que afecta a la eficacia del algoritmo.\n",
    "\n",
    "Para llevar a cabo las predicciones, el algoritmo KNN cl√°sico se basa en los dos siguientes elementos configurables: el n√∫mero $ùëò‚ààùëÅ$ de vecinos m√°s cercanos y la funci√≥n de distancia d. Con respecto a esta √∫ltima, existen numerosas funciones de distancia propuestas en la literatura. En esta secci√≥n usaremos la distancia de Minkowski, la cual se define as√≠:$$Minkowski(ùëí_ùëñ,ùëí_ùëó)=(\\sum_{d=1}^{p}|x_{i,d} - x_{j,d}|^q)^\\frac{1}{q}$$\n",
    "\n",
    "La funci√≥n de distancia es un hiperpar√°metro de KNN que afecta en gran medida a la bondad de las predicciones. Es cierto, que cada problema requiere una funci√≥n de distancia m√°s adecuada a la naturaleza y distribuci√≥n de los datos, pero en la mayor√≠a de los casos, la distancia m√°s utilizada es la eucl√≠dea, cuando el valor de p es igual a 2\n",
    "![Distancia de Minkowski](https://leovan.me/images/cn/2019-01-01-similarity-and-distance-measurement/2D-unit-balls.png)\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La f√°cil interpretabilidad de las predicciones.\n",
    "+ El tiempo de ejecuci√≥n del entrenamiento pr√°cticamente nulo.\n",
    "\n",
    "### Desventajas\n",
    "+ El tiempo de ejecuci√≥n alto en la fase de predicci√≥n.\n",
    "+ La falta de generalizaci√≥n.\n",
    "+ La necesidad de atributos relevantes y en igual escala.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
