{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Algoritmos de regresiÃ³n\n",
    "## Requisitos\n",
    "## IntroducciÃ³n\n",
    "## RegresiÃ³n lineal (OLS)\n",
    "### DefiniciÃ³n\n",
    "Como sabemos, los modelos de conocimiento en aprendizaje supervisado pueden ser entendidos como funciones que reciben una entrada, la cual es un vector con los valores de los atributos, y devuelven una salida, que se trata de un valor estimado para\n",
    "la clase. El algoritmo OLS â€“_ordinary least squares, en inglÃ©s_ â€“es capaz de generar un modelo que consta de una combinaciÃ³n lineal de los atributos. En concreto, el modelo que genera OLS puede expresarse matemÃ¡ticamente del siguiente modo:\n",
    "\n",
    "$$ğ‘¦Ì‚=ğ‘¤_0 + ğ‘¤_1 Â· ğ‘¥_1 + â‹¯ + ğ‘¤_ğ‘ Â· ğ‘¥_ğ‘$$\n",
    "\n",
    "En la anterior ecuaciÃ³n, $ğ‘¦Ì‚$ es el valor predicho por el algoritmo, $x_1, ..., x_p$ son los valores de los atributos, $ğ‘¤_1, ..., ğ‘¤_ğ‘âˆˆğ‘…$ son los coeficientes del modelo y $w_0$ es el tÃ©rmino independiente de la combinaciÃ³n lineal.\n",
    "\n",
    "Una vez explicado el modelo de OLS, vamos a explicar la funciÃ³n de coste que emplea para optimizar los coeficientes del modelo y asÃ­, conseguir que sea capaz de estimar con mayor precisiÃ³n la variable de salida. \n",
    "\n",
    "La funciÃ³n objetivo o funciÃ³n de coste es aquella que evalua cÃ³mo de buena es la posible soluciÃ³n. En el caso de OLS, la funciÃ³n objetivo que usa es [MSE (mean squared error)](https://www.iartificial.net/error-cuadratico-medio-para-regresion/), y consiste en encontrar aquellos valores para los coeficientes $ğ‘¤_0, ..., ğ‘¤_ğ‘$ que minimizan el error. \n",
    "![ReduciciÃ³n del error cuadrÃ¡tico medio](https://miro.medium.com/max/1400/1*1pBRuJzxj4oqwnDx0Ygf1w.png \"ReduciciÃ³n del error cuadrÃ¡tico medio\")\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La fÃ¡cil interpretabilidad del modelo.\n",
    "+ La gran eficacia en problemas de Ã­ndole lineal.\n",
    "+ El tiempo de ejecuciÃ³n del entrenamiento muy razonable.\n",
    "+ El tiempo de ejecuciÃ³n de la predicciÃ³n casi instantÃ¡neo.\n",
    "\n",
    "### Desventajas\n",
    "+ La independencia entre los atributos.\n",
    "+ La distribuciÃ³n normal de los datos.\n",
    "+ La relaciÃ³n lineal entre los atributos y la clase.\n",
    "\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegresiÃ³n logÃ­stica\n",
    "### DefiniciÃ³n\n",
    "La regresiÃ³n logÃ­stica (LOGR) es un algoritmo de aprendizaje automÃ¡tico que se utiliza sobre todo para los problemas de clasificaciÃ³n, aunque tambiÃ©n se puede emplear en problemas de regresiÃ³n. Es un algoritmo de anÃ¡lisis predictivo y se basa en el concepto de probabilidad. \n",
    "\n",
    "LOGR utiliza una funciÃ³n definida como funciÃ³n sigmoide y devuelve un valor entre 0 y 1 dependiendo del valor de entrada que reciba. Pero claro, esto sÃ³lo es Ãºtil si tenemos que predecir dos valores, Â¿pero que pasarÃ­a si tenemos que predecir _n_ valores? Para resolver este problema se lleva a cabo la descomposiciÃ³n del problema original en _m_ subproblemas, del tipo binario (la funciÃ³n sigmoide). A este proceso se le define como _una contra todos o [one vs rest (OvR)](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/)_.\n",
    "\n",
    "La divisiÃ³n en subproblemas binarios de tipo _OvR_ permite realizar una regresiÃ³n usando la regresiÃ³n logÃ­stica mÃ¡s simple. Cada modelo, una vez entrenado, aprenderÃ­a una frontera lineal de decisiÃ³n que enfrenta cada target con los restantes, tal como se muestra en la figura para un problema con 3 targets. \n",
    "\n",
    "![One vs rest (OvR)](../assets/img/OvR.png \"One vs rest (OvR)\")\n",
    "\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La fÃ¡cil interpretabilidad de las predicciones.\n",
    "+ El tiempo de ejecuciÃ³n del entrenamiento muy razonable.\n",
    "+ El tiempo de ejecuciÃ³n de la predicciÃ³n casi instantÃ¡neo.\n",
    "\n",
    "### Desventajas\n",
    "+ La independencia entre los atributos.\n",
    "+ La excesiva simplicidad del modelo.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ãrboles de decisiÃ³n\n",
    "### DefiniciÃ³n\n",
    "### Ventajas\n",
    "### Desventajas\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecinos mÃ¡s cercanos\n",
    "### DefiniciÃ³n\n",
    "El algoritmo de vecinos mÃ¡s cercanos â€“_k nearest neighbors(KNN), en inglÃ©s_ â€“se basa en la idea de que ejemplos similares en sus atributos presentan tambiÃ©n un comportamiento similar en el valor de sus clases. La idea consiste en producir como predicciÃ³n el promedio de las clases de los ejemplos de entrenamiento mÃ¡s parecidos (vecinos) al ejemplo de prueba que hay que predecir. Definiremos quÃ© debe considerarse mÃ¡s parecido, aspecto que afecta a la eficacia del algoritmo.\n",
    "\n",
    "Para llevar a cabo las predicciones, el algoritmo KNN clÃ¡sico se basa en los dos siguientes elementos configurables: el nÃºmero $ğ‘˜âˆˆğ‘$ de vecinos mÃ¡s cercanos y la funciÃ³n de distancia d. Con respecto a esta Ãºltima, existen numerosas funciones de distancia propuestas en la literatura. En esta secciÃ³n usaremos la distancia de Minkowski, la cual se define asÃ­:$$Minkowski(ğ‘’_ğ‘–,ğ‘’_ğ‘—)=(\\sum_{d=1}^{p}|x_{i,d} - x_{j,d}|^q)^\\frac{1}{q}$$\n",
    "\n",
    "La funciÃ³n de distancia es un hiperparÃ¡metro de KNN que afecta en gran medida a la bondad de las predicciones. Es cierto, que cada problema requiere una funciÃ³n de distancia mÃ¡s adecuada a la naturaleza y distribuciÃ³n de los datos, pero en la mayorÃ­a de los casos, la distancia mÃ¡s utilizada es la euclÃ­dea, cuando el valor de p es igual a 2\n",
    "![Distancia de Minkowski](https://leovan.me/images/cn/2019-01-01-similarity-and-distance-measurement/2D-unit-balls.png)\n",
    "\n",
    "### Ventajas\n",
    "+ La simplicidad del modelo.\n",
    "+ La fÃ¡cil interpretabilidad de las predicciones.\n",
    "+ El tiempo de ejecuciÃ³n del entrenamiento prÃ¡cticamente nulo.\n",
    "\n",
    "### Desventajas\n",
    "+ El tiempo de ejecuciÃ³n alto en la fase de predicciÃ³n.\n",
    "+ La falta de generalizaciÃ³n.\n",
    "+ La necesidad de atributos relevantes y en igual escala.\n",
    "\n",
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
